\subsubsection{Limitations of IP3D Algorithm}

The $b$-hadron decay gives birth to a number of charged particles with large impact parameters emerging from the same secondary vectex. Given they have the same origin, these track impact parameters are correlated. Given the jet is a $b$-jet and there is a track with large impact parameter, the probability of finding some other high impact parameter trakcs in the jet is high, while the light-jet track impact parameters shall be independent. The 2D distribution of transverse impact parameter significance (\sdip) for the leading and subleading $|\sdip|$ tracks are shown in Figure~\ref{fig:ip_corr} for $b$-jets, where a correlation can clearly be seen, and light flavour-jets, where no such correlation is observed.

\begin{figure}[htbp]
  \centering
   \includegraphics[width=0.48\textwidth]{figures/RNN/Sd0_2d_B.pdf}
 \includegraphics[width=0.48\textwidth]{figures/RNN/Sd0_2d_L.pdf}
\caption{The distribution of the \sdip for the leading and subleading $|\sdip|$ significance track in $b$-jets (left) and light jets (right). }
  \label{fig:ip_corr}
\end{figure}


The baseline IP3D $b$-tagging algorithm, uses 3D likelihood templates in $\sdip$, $\szip$, and a track categorization to compute three per-flavor conditional likelihoods, $p_b$, $p_c$, and $p_{\textrm{light}}$. These likelihood templates are derived from histograms with 35 bins in $\sdip$, 20 bins in $\szip$, and 14 bins in track category, where each category corresponds to a different track quality~\cite{ATL-PHYS-PUB-2015-022}. Direct estimate of the joint probability distribution of these three variables is not possible, as the joint distribution has a total bin count of $35 \times 20 \times 14 \times 3 = 29,400$ if we want to maintain the same resolution as for the marginal distribution of each variable.In addition, extending the template to account for additional kinematic variables and the variable number of tracks within the jet is even more computationally expensive, as the number of template bins grows exponentially.

To overcome this difficulty, the IP3D algorithm made the assumption that is that the per-track flavor conditional likelihood can be computed independent of the other tracks in the jet.  Such a likelihood model does not take into account correlations amongst track parameters, and the method of building templates to define likelihoods requires large sample sizes. Technicially, the IP3D algorithm adopts a Naive Bayes estimate was made, the likelihood of a jet being of a given flavour is computed as the product of the per-track likelihoods which are in turn estimated by the product of per-track per-variable likelihohods . The IP3D discriminant is built from the conditional log-likelihood ratio, $\textrm{IP3D}=\ln \prod_{i \in \textrm{tracks}} p_b^i / p_{\textrm{light}}^i$. 

\subsubsection{Recurrent Networks}

Recurrent neural networks are used to directly learn sequence-based dependencies for arbitrary-length inputs with strong ordering~\cite{ref:RNNthesis, dlbook}. The fundamental unit of an RNN is a cell encapsulating an internal state vector. As the first step of processing any given sequence (in this case the tracks in a jet), the internal state is initialized to zero. At each step in the sequence, the cell is handed a fixed number of inputs (in this case the parameters that describe one track). These parameters are combined with the \emph{current} internal state in order to compute a \emph{new} internal state based on a set of rules which are tuned in the training phase. At the end of the sequence the cell's internal state serves as a fixed-dimensional representation of the entire sequence. In this way a recurrent cell is able to reduce a sequence of arbitrary length to a fixed number of variables, which can then be processed by a traditional feed-forward network.\footnote{For a review of terminology such as ``feed-forward'', ``fully-connected'', and ``softmax'' and a more pedagogical introduction to deep learning, see for instance ~\cite{dlbook,2014arXiv1404.7828S}.}

Much of the recent success of RNNs in various natural language and long-sequence processing applications can be attributed to the advent of Long Short-Term Memory (LSTM)~\cite{ref:LSTM} units and later variants such as Gated Recurrent Units (GRUs)~\cite{ref:GRU,cho14}. These architectural modifications at the cell level mitigate issues related to vanishing and exploding gradients~\cite{hochreiter1991untersuchungen,Bengio:1994:LLD:2325857.2328340,DBLP:journals/corr/abs-1211-5063}, and improve the knowledge persistence of long-term dependencies. These special kinds of recurrent units employ different internal gating mechanisms to modify the cell state in order to balance and regulate the relative importance of long-term and short-term information.
